---
title: "621 HW 1 v 4"
author: "Jeff Nieman, Scott Karr, James Topor, Armenoush"
date: "June 13, 2016"
output: pdf_document
---

```{r load-libaries, eval=TRUE, include=FALSE}
library(car)
library(fBasics)
library(knitr)
library(car)
library(corrplot)
library(alr3)
library(MASS)
```
#CONSOLIDATE VARIABLES, REMOVE NA'S & OUTLIERS
#####Create new column for batting singles and eliminating hits for batting
```{r, include=FALSE}
mb_e <- read.csv("https://raw.githubusercontent.com/jtopor/CUNY-MSDA-621/master/HW-1/moneyball-training-data.csv")  
#eliminate index column
mb_e1 <- mb_e[,-1]

#add singles column for hitting
mb_e1$TEAM_BATTING_1B <- as.numeric(mb_e1$TEAM_BATTING_H-mb_e1$TEAM_BATTING_2B-mb_e1$TEAM_BATTING_3B-mb_e1$TEAM_BATTING_HR)
mb_e1 <- mb_e1[,-2]
mb_e1 <- as.data.frame(mb_e1)
```

#####Eliminate HBP, CS, and pitching HR's.
```{r, include=FALSE}
mb <- mb_e1[,-c(8,9,11)]
summary(mb)
```


#####Build model for batting SO using Gelman approach
```{r, include=FALSE}
BSO <- lm(data=mb, TEAM_BATTING_SO~.)
summary(BSO)

#take out double plays + pitching SO + SB as data set is incomplete + Wins as they are not present in the evaluation data
BSO.1 <- lm(data=mb, TEAM_BATTING_SO~. -TEAM_FIELDING_DP -TEAM_PITCHING_SO -TEAM_BASERUN_SB -TARGET_WINS)
summary(BSO.1)

#eliminate doubles
BSO.2 <- lm(data=mb, TEAM_BATTING_SO~. -TEAM_FIELDING_DP -TEAM_PITCHING_SO -TEAM_BASERUN_SB - TARGET_WINS -TEAM_BATTING_2B)
summary(BSO.2)

##All p-values are low with a 686.8 F-statistic and adjusted R squared of 0.7236
#take a look
par(mfrow=c(2,2))
plot(BSO.2)
plot(BSO.2$residuals)

#prediction function
pred.BSO <- round(predict(BSO.2, mb))
impute <- function (a, a.impute){
  ifelse (is.na(a), a.impute,a)
}

BSO.imp <- impute(mb$TEAM_BATTING_SO, pred.BSO)

###################################################
# Jims added code for diagnostics of imputation

# first, check summaries to ensure similar values
summary(mb$TEAM_BATTING_SO)
summary(BSO.imp)

# now plot side-by-side histograms to check similarity of distributions
par(mfrow = c(2,2))
hist(mb$TEAM_BATTING_SO, breaks = 200)
hist(BSO.imp, breaks = 200)
###################################################

#place back in the data base with imputed data for SO's
mb1 <- mb
mb1$TEAM_BATTING_SO <- BSO.imp
```

#####Build model for Pitching SO
```{r,include=FALSE}
PSO <- lm(data=mb1, TEAM_PITCHING_SO~.)
summary(PSO)

#take out double plays + SB as data set is incomplete and wins as they are not present in evaluation data
PSO.1 <- lm(data=mb1, TEAM_PITCHING_SO~. -TEAM_FIELDING_DP -TEAM_BASERUN_SB - TARGET_WINS)
summary(PSO.1)

#all low P value and F statistic of 48090 with adj R squared of 0.9958
#take a look
par(mfrow=c(2,2))
plot(PSO.1)
plot(PSO.1$residuals)

#place back in the data base with imputed data for SO's
pred.PSO <- round(predict(PSO.1, mb1))
PSO.imp <- impute(mb1$TEAM_PITCHING_SO, pred.PSO)

###################################################
# Jims added code for diagnostics of imputation

# first, check summaries to ensure similar values
summary(mb1$TEAM_PITCHING_SO)
summary(PSO.imp)

# now plot side-by-side histograms to check similarity of distributions
par(mfrow = c(2,2))
hist(mb1$TEAM_PITCHING_SO, breaks = 200)
hist(PSO.imp, breaks = 200)
###################################################

mb2 <- mb1
mb2$TEAM_PITCHING_SO <- PSO.imp
```

#####Build model for SB
```{r, include=FALSE}
SB <- lm(data=mb2, TEAM_BASERUN_SB~.)
summary(SB)

#Take out DP as incomplete data and target wins
SB.1 <- lm(data=mb2, TEAM_BASERUN_SB~.-TEAM_FIELDING_DP - TARGET_WINS)
summary(SB.1)

#eliminate pitching BB's
SB.2 <- lm(data=mb2, TEAM_BASERUN_SB~.-TEAM_FIELDING_DP -TEAM_PITCHING_BB - TARGET_WINS)
summary(SB.2)

#eliminate singles
SB.3 <- lm(data=mb2, TEAM_BASERUN_SB~.-TEAM_FIELDING_DP -TEAM_PITCHING_BB -TEAM_BATTING_1B - TARGET_WINS)
summary(SB.3)

#simplify the model by taking out pitching
SB.4 <- lm(data=mb2, TEAM_BASERUN_SB~.-TEAM_FIELDING_DP -TEAM_PITCHING_BB -TEAM_BATTING_1B - TARGET_WINS - TEAM_PITCHING_SO - TEAM_PITCHING_H)
summary(SB.4)

#add singles back in
SB.5 <- lm(data=mb2, TEAM_BASERUN_SB~.-TEAM_FIELDING_DP -TEAM_PITCHING_BB - TARGET_WINS - TEAM_PITCHING_SO - TEAM_PITCHING_H)
summary(SB.5)

#eliminate doubles
SB.6 <- lm(data=mb2, TEAM_BASERUN_SB~.-TEAM_FIELDING_DP -TEAM_PITCHING_BB - TARGET_WINS - TEAM_PITCHING_SO - TEAM_PITCHING_H - TEAM_BATTING_2B)
summary(SB.6)

#eliminate walks
SB.7 <- lm(data=mb2, TEAM_BASERUN_SB~.-TEAM_FIELDING_DP -TEAM_PITCHING_BB - TARGET_WINS - TEAM_PITCHING_SO - TEAM_PITCHING_H - TEAM_BATTING_2B - TEAM_BATTING_BB)
summary(SB.7)

#all low P value and F statistic of 202.1 with adj R squared of 0.3418
#take a look
par(mfrow=c(2,2))
plot(SB.7)
plot(SB.7$residuals)

#place back in the data base with imputed data for SB's
pred.SB <- round(predict(SB.7, mb2))
SB.imp <- impute(mb2$TEAM_BASERUN_SB, pred.SB)

###################################################
# Jims added code for diagnostics of imputation

# first, check summaries to ensure similar values
summary(mb2$TEAM_BASERUN_SB)
summary(SB.imp)

# now plot side-by-side histograms to check similarity of distributions
par(mfrow = c(2,2))
hist(mb2$TEAM_BASERUN_SB, breaks = 200)
hist(SB.imp, breaks = 200)
###################################################

mb3 <- mb2
mb3$TEAM_BASERUN_SB <- SB.imp
```

#####Build model to replace DP
```{r, include=FALSE}
DP <- lm(data=mb3, TEAM_FIELDING_DP~.)
summary(DP)

#remove target wins
DP.1 <- lm(data=mb3, TEAM_FIELDING_DP~.-TARGET_WINS)
summary(DP.1)

#remove batting 2B's
DP.2 <- lm(data=mb3, TEAM_FIELDING_DP~.-TARGET_WINS - TEAM_BATTING_2B)
summary(DP.2)
# results show that EVERYTHING ELSE is statistically signficant, so:

# run vif to check for collinearity
vif(DP.2)
# results show TEAM_BATTING_SO should be removed

# remove TEAM_BATTING_SO
DP.3 <- lm(data=mb3, TEAM_FIELDING_DP~. -TARGET_WINS -TEAM_BATTING_2B - TEAM_BATTING_SO)
summary(DP.3)
vif(DP.3)
# p-value says remove TEAM_PITCHING_SO; vif says remove TEAM_BATTING_BB

# remove TEAM_PITCHING_SO
DP.4 <- lm(data=mb3, TEAM_FIELDING_DP~.-TEAM_BATTING_2B -TARGET_WINS -TEAM_BATTING_2B - TEAM_BATTING_SO - TEAM_PITCHING_SO)
summary(DP.4)
vif(DP.4)
# P values and vif both indicate remove TEAM_PITCHING_BB

# remove TEAM_PITCHING_BB
DP.5 <- lm(data=mb3, TEAM_FIELDING_DP~. -TARGET_WINS -TEAM_BATTING_2B - TEAM_BATTING_SO - TEAM_PITCHING_SO - TEAM_PITCHING_BB)
summary(DP.5)
vif(DP.5)
# vif says remove TEAM_FIELDING_E; p-values all < .05 so remove TEAM_FIELDING_E

DP.6 <- lm(data=mb3, TEAM_FIELDING_DP~. -TARGET_WINS -TEAM_BATTING_2B - TEAM_BATTING_SO - TEAM_PITCHING_SO - TEAM_PITCHING_BB - TEAM_FIELDING_E)
summary(DP.6)
vif(DP.6)
# now no collinearity but p-values say remove TEAM_PITCHING_H

DP.7 <- lm(data=mb3, TEAM_FIELDING_DP~. -TARGET_WINS -TEAM_BATTING_2B - TEAM_BATTING_SO - TEAM_PITCHING_SO - TEAM_PITCHING_BB - TEAM_FIELDING_E - TEAM_PITCHING_H)
summary(DP.7)
vif(DP.7)
# no collinearity, all p-values < .05 so stop

######################################### STOP HERE ##################################
# THIS IS THE CODE FROM THE ORIGINAL FORMULATION

#remove batting 1B's
# DP.3 <- lm(data=mb3, TEAM_FIELDING_DP~.-TEAM_BATTING_2B -TARGET_WINS - TEAM_BATTING_1B)
# summary(DP.3)

#remove batting 3B's
# DP.4 <- lm(data=mb3, TEAM_FIELDING_DP~.-TEAM_BATTING_2B -TEAM_BATTING_3B - TEAM_BATTING_1B - TARGET_WINS)
# summary(DP.4)

######################################################################################

#all low P value and F statistic of 174.7 with adj R squared of 0.4113
#take a look
par(mfrow=c(2,2))
plot(DP.7)
plot(DP.7$residuals)

#place back in the data base with imputed data for SB's
# NOTE: Changed DP.4 to DP.7 here
pred.DP <- round(predict(DP.7, mb3))
DP.imp <- impute(mb3$TEAM_FIELDING_DP, pred.DP)

###################################################
# Jims added code for diagnostics of imputation

# first, check summaries to ensure similar values
summary(mb3$TEAM_FIELDING_DP)
summary(DP.imp)

# now plot side-by-side histograms to check similarity of distributions
par(mfrow = c(2,2))
hist(mb3$TEAM_FIELDING_DP, breaks = 200)
hist(DP.imp, breaks = 200)
###################################################

mb4 <- mb3
mb4$TEAM_FIELDING_DP <- DP.imp

#test new data set
summary(mb4)
```


#####Eliminate unhistorical outliers
```{r, include=FALSE}
# check rowcount before removal of outliers
nrow(mb4)

############## TEAM PITCHING_SO ############################
#most pitching SO's is 1450.  So delete all records with more than 1450 pitching SO's
mb5 <- mb4

# fixed error in this line: dataframe in 'which' call was mb1 so changed to mb5
mb5 <- mb5[which(mb5$TEAM_PITCHING_SO < 1450),]
summary(mb5)

# check rowcount
nrow(mb5)

############ TEAM_PITCHING_H ##############################
#most ever hits by a team is 1730.  So delete all pitching hits >3000 to be conservative with the median
mb6 <- mb5
mb6 <- mb6[which(mb6$TEAM_PITCHING_H < 3001),]
summary(mb6)

# check rowcount
nrow(mb6)

############ TEAM_FIELDING_E ##############################
#most ever errors by a team is 639 by 1883 Philadelphia.  Prorating to 162 games gives a value of 1046.
#Pre WW II : 1883 Philadelphia (NL), 639.
#Post WW II : 1945 Philadelphia (NL), 234 or 1946 Washington (AL), 211.
mb7 <- mb6
mb7 <- mb7[which(mb7$TEAM_FIELDING_E < 1047),]
summary(mb7)

# check rowcount: result is 2172 => removed total of 104 rows
nrow(mb7)

dim(mb)-dim(mb7)

#we removed 104 rows total due to outliers.
```
# SINGLE PREDICTOR ANALYSIS & TRANSFORMATIONS
#####Model SMK Generlized Equation
Review descriptive statistics to confirm each variable is within acceptable bounds and 
contains no missing data.  Review Density plots of 13 variables for skewness to identify
which may require transformation.  

```{r full, eval = TRUE,include=FALSE, echo=FALSE}
#assign model to "clean" data set 
lm.smk <- mb7

#SINGLES:
par(mfrow=c(2,3))
mW  <- lm.smk$TARGET_WINS
mb.1B <- lm.smk$TEAM_BATTING_1B
m1 <- lm(mW~mb.1B, data = lm.smk)
StanRes1 <- rstandard(m1)
plot(density(mb.1B),main="Singles");rug(mb.1B)
plot(mb.1B,StanRes1,xlab="Singles",ylab="Standardized Residuals");abline(lsfit(mb.1B,StanRes1),lty=2,col=2)
qqnorm(mb.1B,ylab="Y");qqline(mb.1B,lty=2,col=2)
##TRANSFORMATION
lambda <- powerTransform(mb.1B, family="bcPower")$lambda
tmb.1B <- mb.1B^(lambda)
plot(density(tmb.1B),main="Singles");rug(tmb.1B)
plot(tmb.1B,StanRes1,xlab="Singles",ylab="Standardized Residuals");abline(lsfit(tmb.1B,StanRes1),lty=2,col=2)
qqnorm(tmb.1B,ylab="Y");qqline(tmb.1B,lty=2,col=2)

#DOUBLES:
par(mfrow=c(2,3))
mW  <- lm.smk$TARGET_WINS
mb.2B <- lm.smk$TEAM_BATTING_2B
m1 <- lm(mW~mb.2B, data = lm.smk)
StanRes1 <- rstandard(m1)
plot(density(mb.2B),main="Doubles");rug(mb.2B)
plot(mb.2B,StanRes1,xlab="Doubles",ylab="Standardized Residuals");abline(lsfit(mb.2B,StanRes1),lty=2,col=2)
qqnorm(mb.2B,ylab="Y");qqline(mb.2B,lty=2,col=2)
##TRANSFORMATION
lambda <- powerTransform(mb.2B, family="bcPower")$lambda
tmb.2B <- mb.2B^(lambda)
plot(density(tmb.2B),main="Doubles");rug(tmb.2B)
plot(tmb.2B,StanRes1,xlab="Doubles",ylab="Standardized Residuals");abline(lsfit(tmb.2B,StanRes1),lty=2,col=2)
qqnorm(tmb.2B,ylab="Y");qqline(tmb.2B,lty=2,col=2)

#TRIPLES:
par(mfrow=c(2,3))
mW  <- lm.smk$TARGET_WINS
mb.3B <- lm.smk$TEAM_BATTING_3B
m1 <- lm(mW~mb.3B, data = lm.smk)
StanRes1 <- rstandard(m1)
plot(density(mb.3B),main="Triples");rug(mb.3B)
plot(mb.3B,StanRes1,xlab="Triples",ylab="Standardized Residuals");abline(lsfit(mb.3B,StanRes1),lty=2,col=2)
qqnorm(mb.3B,ylab="Y");qqline(mb.3B,lty=2,col=2)
##TRANSFORMATION
lambda <- powerTransform(mb.3B, family="bcPower")$lambda
tmb.3B <- mb.3B^(lambda)
plot(density(tmb.3B),main="Triples");rug(tmb.3B)
plot(tmb.3B,StanRes1,xlab="Triples",ylab="Standardized Residuals");abline(lsfit(tmb.3B,StanRes1),lty=2,col=2)
qqnorm(tmb.3B,ylab="Y");qqline(tmb.3B,lty=2,col=2)

#HOMERUNS:
par(mfrow=c(2,3))
mW  <- lm.smk$TARGET_WINS
mb.HR <- lm.smk$TEAM_BATTING_HR
m1 <- lm(mW~mb.HR, data = lm.smk)
StanRes1 <- rstandard(m1)
plot(density(mb.HR),main="Homeruns");rug(mb.HR)
plot(mb.HR,StanRes1,xlab="Homeruns",ylab="Standardized Residuals");abline(lsfit(mb.HR,StanRes1),lty=2,col=2)
qqnorm(mb.HR,ylab="Y");qqline(mb.HR,lty=2,col=2)
##TRANSFORMATION
lambda <- powerTransform(mb.HR, family="bcPower")$lambda
tmb.HR <- mb.HR^(lambda)
plot(density(tmb.HR),main="Homeruns");rug(tmb.HR)
plot(tmb.HR,StanRes1,xlab="Homeruns",ylab="Standardized Residuals");abline(lsfit(tmb.HR,StanRes1),lty=2,col=2)
qqnorm(tmb.HR,ylab="Y");qqline(tmb.HR,lty=2,col=2)

#WALKS:
par(mfrow=c(2,3))
mW  <- lm.smk$TARGET_WINS
mb.BB <- lm.smk$TEAM_BATTING_BB
m1 <- lm(mW~mb.BB, data = lm.smk)
StanRes1 <- rstandard(m1)
plot(density(mb.BB),main="Walks");rug(mb.BB)
plot(mb.BB,StanRes1,xlab="Walks",ylab="Standardized Residuals");abline(lsfit(mb.BB,StanRes1),lty=2,col=2)
qqnorm(mb.BB,ylab="Y");qqline(mb.BB,lty=2,col=2)
##TRANSFORMATION
lambda <- powerTransform(mb.BB, family="bcPower")$lambda
tmb.BB <- mb.BB^(lambda)
plot(density(tmb.BB),main="Walks");rug(tmb.BB)
plot(tmb.BB,StanRes1,xlab="Walks",ylab="Standardized Residuals");abline(lsfit(tmb.BB,StanRes1),lty=2,col=2)
qqnorm(tmb.BB,ylab="Y");qqline(tmb.BB,lty=2,col=2)

#STRIKEOUTS:
par(mfrow=c(2,3))
mW  <- lm.smk$TARGET_WINS
mb.SO <- lm.smk$TEAM_BATTING_SO
m1 <- lm(mW~mb.SO, data = lm.smk)
StanRes1 <- rstandard(m1)
plot(density(mb.SO),main="Strikeouts");rug(mb.SO)
plot(mb.SO,StanRes1,xlab="Strikeouts",ylab="Standardized Residuals");abline(lsfit(mb.SO,StanRes1),lty=2,col=2)
qqnorm(mb.SO,ylab="Y");qqline(mb.SO,lty=2,col=2)
##TRANSFORMATION
lambda <- powerTransform(mb.SO, family="bcPower")$lambda
tmb.SO <- mb.SO^(lambda)
plot(density(tmb.SO),main="Strikeouts");rug(tmb.SO)
plot(tmb.SO,StanRes1,xlab="Strikeouts",ylab="Standardized Residuals");abline(lsfit(tmb.SO,StanRes1),lty=2,col=2)
qqnorm(tmb.SO,ylab="Y");qqline(tmb.SO,lty=2,col=2)

#STOLEN BASES:
par(mfrow=c(2,3))
mW  <- lm.smk$TARGET_WINS
mb.SB <- lm.smk$TEAM_BASERUN_SB
m1 <- lm(mW~mb.SB, data = lm.smk)
StanRes1 <- rstandard(m1)
plot(density(mb.SB),main="Stolen Bases");rug(mb.SB)
plot(mb.SB,StanRes1,xlab="Stolen Bases",ylab="Standardized Residuals");abline(lsfit(mb.SB,StanRes1),lty=2,col=2)
qqnorm(mb.SB,ylab="Y");qqline(mb.SB,lty=2,col=2)
##TRANSFORMATION
lambda <- powerTransform(mb.SB, family="bcPower")$lambda
tmb.SB <- mb.SB^(lambda)
plot(density(tmb.SB),main="Stolen Bases");rug(tmb.SB)
plot(tmb.SB,StanRes1,xlab="Stolen Bases",ylab="Standardized Residuals");abline(lsfit(tmb.SB,StanRes1),lty=2,col=2)
qqnorm(tmb.SB,ylab="Y");qqline(tmb.SB,lty=2,col=2)

#PITCHING HITS:
par(mfrow=c(2,3))
mW  <- lm.smk$TARGET_WINS
mp.H <- lm.smk$TEAM_PITCHING_H
m1 <- lm(mW~mp.H, data = lm.smk)
plot(density(mp.H),main="Pitching Hits");rug(mp.H)
plot(mp.H,StanRes1,xlab="Pitching Hits",ylab="Standardized Residuals");abline(lsfit(mp.H,StanRes1),lty=2,col=2)
qqnorm(mp.H, ylab = "Y");qqline(mp.H, lty = 2, col=2)
##TRANSFORMATION
lambda <- powerTransform(mp.H,family="bcPower")$lambda
tmp.H <- mp.H^(lambda)
plot(density(tmp.H),main="Pitching Hits");rug(tmp.H)
plot(tmp.H,StanRes1,xlab="Pitching Hits",ylab="Standardized Residuals");abline(lsfit(tmp.H,StanRes1),lty=2,col=2)
qqnorm(tmp.H,ylab="Y");qqline(tmp.H,lty=2,col=2)

#PITCHING WALKS:
par(mfrow=c(2,3))
mW  <- lm.smk$TARGET_WINS
mp.BB <- lm.smk$TEAM_PITCHING_BB
m1 <- lm(mW~mp.BB, data = lm.smk)
plot(density(mp.BB),main="Pitching Walks");rug(mp.BB)
plot(mp.BB,StanRes1,xlab="Pitching Walks",ylab="Standardized Residuals");abline(lsfit(mp.BB,StanRes1),lty=2,col=2)
qqnorm(mp.BB, ylab = "Y");qqline(mp.BB,lty = 2,col=2)
##TRANSFORMATION
lambda <- powerTransform(mp.BB, family="bcPower")$lambda
tmp.BB <- mp.BB^(lambda)
plot(density(tmp.BB),main="Pitching Walks");rug(tmp.BB)
plot(tmp.BB,StanRes1,xlab="Pitching Walks",ylab="Standardized Residuals");abline(lsfit(tmp.BB,StanRes1),lty=2,col=2)
qqnorm(tmp.BB,ylab="Y");qqline(tmp.BB,lty=2,col=2)

#PITCHING STRIKEOUTS:
par(mfrow=c(2,3))
mW  <- lm.smk$TARGET_WINS
mp.SO <- lm.smk$TEAM_PITCHING_SO
m1 <- lm(mW~mp.SO, data = lm.smk)
plot(density(mp.SO),main="Pitching Strikeouts");rug(mp.SO)
plot(mp.SO,StanRes1,xlab="Pitching Strikeouts",ylab="Standardized Residuals");abline(lsfit(mp.SO,StanRes1),lty=2,col=2)
qqnorm(mp.SO, ylab = "Y");qqline(mp.SO,lty = 2,col=2)
##TRANSFORMATION
lambda <- powerTransform(mp.SO, family="bcPower")$lambda
tmp.SO <- mp.SO^(lambda)
plot(density(tmp.SO),main="Pitching Strikeouts");rug(tmp.SO)
plot(tmp.SO,StanRes1,xlab="Pitching Strikeouts",ylab="Standardized Residuals");abline(lsfit(tmp.SO,StanRes1),lty=2,col=2)
qqnorm(tmp.SO,ylab="Y");qqline(tmp.SO,lty=2,col=2)

#FIELDING ERRORS:
par(mfrow=c(2,3))
mW   <- lm.smk$TARGET_WINS
mf.E <- lm.smk$TEAM_FIELDING_E
m1 <- lm(mW~mf.E, data = lm.smk)
plot(density(mf.E),main="Fielding Errors");rug(mf.E)
plot(mf.E,StanRes1,xlab="Fielding Errors",ylab="Standardized Residuals");abline(lsfit(mf.E,StanRes1),lty=2,col=2)
qqnorm(mf.E, ylab = "Y");qqline(mf.E, lty = 2, col=2)
##TRANSFORMATION
lambda <- powerTransform(mp.H,family="bcPower")$lambda
tmf.E <- mf.E^(lambda)
plot(density(tmf.E),main="Fielding Errors");rug(tmf.E)
plot(tmf.E,StanRes1,xlab="Fielding Errors",ylab="Standardized Residuals");abline(lsfit(tmf.E,StanRes1),lty=2,col=2)
qqnorm(tmf.E,ylab="Y");qqline(tmf.E,lty=2,col=2)

#DOUBLE PLAYS:
par(mfrow=c(2,3))
mW   <- lm.smk$TARGET_WINS
mf.DP <- lm.smk$TEAM_FIELDING_DP
m1 <- lm(mW~mf.DP, data = lm.smk)
plot(density(mf.DP),main="Fielding Doubleplays");rug(mf.DP)
plot(mf.DP,StanRes1,xlab="Fielding Doubleplays",ylab="Standardized Residuals");abline(lsfit(mf.DP,StanRes1),lty=2,col=2)
qqnorm(mf.DP, ylab = "Y");qqline(mf.DP,lty = 2, col=2)
##TRANSFORMATION
lambda <- powerTransform(mf.DP,family="bcPower")$lambda
tmf.DP <- mf.DP^(lambda)
plot(density(tmf.DP),main="Fielding Doubleplays");rug(tmf.DP)
plot(tmf.DP,StanRes1,xlab="Fielding Doubleplays",ylab="Standardized Residuals");abline(lsfit(tmf.DP,StanRes1),lty=2,col=2)
qqnorm(tmf.DP,ylab="Y");qqline(tmf.DP,lty=2,col=2)

par(mfrow=c(1,1))
```

##### Evaluate Correlations  
Evaluate Correlation between predictors so as to not introduce collinearity into the model.
```{r correlation, eval=TRUE, echo=FALSE}
# assign model one "lm.1" to data set with all NA's imputed and "bad" leverage points removed
lm.smk <- mb7
par(cex = 0.65)
corrplot(
    cor(lm.smk[c(1:13)]), 
    type='lower', 
    tl.srt=45,
    addshade="positive",
    addCoef.col = rgb(0,0,0,alpha=0.3),
    addCoefasPercent = TRUE)
```

##### Model Selection Strategy
Two common strategies for adding or removing variables in a multiple regression model are
called backward elimination and forward selection. These techniques are often referred to
as stepwise model selection strategies, because they add or delete one variable at a time
as they “step” through the candidate predictors.  Model 1 uses the forward selection strategy 
which adds variables one-at-a-time until variables cannot be found that improve the model 
as measured by adjusted $R^2$.
Diez, D.M., Barr, C.D., & Çetinkaya-Rundel, M. (2015). OpenIntro Statistics (3rd Ed). pg. 378

#####Start with p.Hits & p.Walks
\[
\begin{aligned}
\widehat{wins} &= \hat{\beta}_0 + 
                    \hat{\beta}_1 \times p.Hits + 
                    \hat{\beta}_2 \times p.Walks 
                    \end{aligned}
\]

```{r m1-iteration1,eval=TRUE,include=FALSE}
#VARIABLES
#variables have been transformed first as individual predictors
Wins  <- mW
p.Hits <- tmp.H 
p.Walks <- tmp.BB
m1 <- lm(Wins ~ p.Hits+p.Walks)

#PAIRWISE PLOT
par(mfrow=c(1,1))
pairs(Wins ~ p.Hits+p.Walks)

#MODEL DIAGNOSTICS
summary(m1)

#diagnostic 1. show collinearity of variables after checking p-values to < 0.05.
vif(m1)

#If resulting coefficients are > 5, remove the variable w/ largest VIF and re-run the model. 
#Subsequently remove variables either due to p-values becoming > 0.05 or VIF coefficients > 5. 
#Continue until all remaining p-values are < 0.05 and no VIFs  > 5

#p-values are all < 0.05 and no VIFs  > 5

#diagnostic 2.  generate Added Variable Plots: should show linear relationship between response & predictors:
par(mfrow=c(2,2))
avPlots(m1, ~.,ask=FALSE, id.n = 2)

#relationship is linear

#diagnostic 3.  generate Summary Diagnostic Plots
par(mfrow=c(2,2))
plot(m1)
#Upper Left plot "Residuals vs Fitted" 
#       Is there a clear predictable pattern in that plot?  If no model isn't valid
#       Do residuals have uniform variability for all fitted values?  If no model isn't valid.
#Upper Right 
#       Is there normality in your residuals? If not, the model lack normality and the model isn't valid.
#Lower Right plot "Residuals vs. Leverage"
#       The Y-axis in this plot represents standard deviations from the mean of the #residuals. 
#       If most residuals are within 2 standard deviations of mean, the model is likely valid. 
#       The plot also shows #"Cooks Distance" -- a metric for identifying high leverage outliers. 
#       The worst outliers are automatically labeled by rownum in the plot. If the outliers are 
#       far outside the 2 standard deviations then remove those rows from your data set, renumber 
#       the #rows of the resulting dataframe so there are no gaps in the integer row numbers, and 
#       then re-run your model to see if it improves.

# normal distribution, and uniform distribution of residuals
# no significant leverage points

#diagnostic 4.  generate Standardized Residual Plots against each predictor
par(mfrow=c(2,2))
StanRest <- rstandard(m1)
plot(p.Hits,StanRest,ylab="Standardized Residuals")
plot(p.Walks,StanRest,ylab="Standardized Residuals")
plot(m1$fitted.values,StanRest,ylab="Standardized Residuals",xlab="Fitted Values")

#Examine plots for constant variability of residuals across ALL of the values of your predictor. 
#       Constant variability means residuals are disbursed uniformly across all predictor variable's values. 
#       You should not see any predictable pattern or model lacks constant variability and isn't valid.

# uniform distribution of residuals

#diagnostic 5.  generate plot of Y "response variable"" against Fitted Values "regression model"
par(mfrow = c(2,2))
plot(m1$fitted.values,Wins,xlab="Fitted Values",ylab=expression(Wins^lambda))
abline(lsfit(m1$fitted.values,Wins))
plot(m1)
#If plot doesn't shows a linear relationship with no pattern or skew the model lacks normality.

# normal distribution, and uniform distribution of residuals
```

#####Add b.Singles & b.Doubles
\[
\begin{aligned}
\widehat{wins} &= \hat{\beta}_0 + 
                    \hat{\beta}_1 \times p.Hits + 
                    \hat{\beta}_2 \times p.Walks 
                    \hat{\beta}_3 \times b.Singles + 
                    \hat{\beta}_4 \times b.Doubles
                    \end{aligned}
\]

```{r m1-interation2,eval=TRUE,include=FALSE}
#VARIABLES
#variables have been transformed first as individual predictors
Wins  <- mW
p.Hits <- tmp.H 
p.Walks <- tmp.BB
b.Singles <- tmb.1B
b.Doubles <- tmb.2B
m1 <- lm(Wins ~ p.Hits+p.Walks+b.Singles+b.Doubles)

#PAIRWISE PLOT
par(mfrow=c(1,1))
pairs(Wins ~ p.Hits+p.Walks+b.Singles+b.Doubles)

#MODEL DIAGNOSTICS
summary(m1)

#diagnostic 1. show collinearity of variables after checking p-values to < 0.05.
vif(m1)

#If resulting coefficients are > 5, remove the variable w/ largest VIF and re-run the model. 
#Subsequently remove variables either due to p-values becoming > 0.05 or VIF coefficients > 5. 
#Continue until all remaining p-values are < 0.05 and no VIFs  > 5

#p-values of p.Hits >  0.05 so it gets removed
```

#####Removed p.Hits
\[
\begin{aligned}
\widehat{wins} &= \hat{\beta}_0 + 
                    \hat{\beta}_1 \times p.Walks 
                    \hat{\beta}_2 \times b.Singles + 
                    \hat{\beta}_3 \times b.Doubles
                    \end{aligned}
\]

```{r m1-interation3,eval=TRUE,include=FALSE}
#VARIABLES
#variables have been transformed first as individual predictors
Wins  <- mW
p.Walks <- tmp.BB
b.Singles <- tmb.1B
b.Doubles <- tmb.2B
m1 <- lm(Wins ~ p.Walks+b.Singles+b.Doubles)

#PAIRWISE PLOT
par(mfrow=c(1,1))
pairs(Wins ~ p.Walks+b.Singles+b.Doubles)

#MODEL DIAGNOSTICS
summary(m1)

#diagnostic 1. show collinearity of variables after checking p-values to < 0.05.
vif(m1)

#p-values are all < 0.05 and no VIFs  > 5 and adjusted $R^2$ increased

#If resulting coefficients are > 5, remove the variable w/ largest VIF and re-run the model. 
#Subsequently remove variables either due to p-values becoming > 0.05 or VIF coefficients > 5. 
#Continue until all remaining p-values are < 0.05 and no VIFs  > 5
#p-values are all < 0.05 and no VIFs  > 5

#diagnostic 2.  generate Added Variable Plots: should show linear relationship between response & predictors:
par(mfrow=c(2,2))
avPlots(m1, ~.,ask=FALSE, id.n = 2)

#relationship is linear

#diagnostic 3.  generate Summary Diagnostic Plots
par(mfrow=c(2,2))
plot(m1)
#Upper Left plot "Residuals vs Fitted" 
#       Is there a clear predictable pattern in that plot?  If no model isn't valid
#       Do residuals have uniform variability for all fitted values?  If no model isn't valid.
#Upper Right 
#       Is there normality in your residuals? If not, the model lack normality and the model isn't valid.
#Lower Right plot "Residuals vs. Leverage"
#       The Y-axis in this plot represents standard deviations from the mean of the #residuals. 
#       If most residuals are within 2 standard deviations of mean, the model is likely valid. 
#       The plot also shows #"Cooks Distance" -- a metric for identifying high leverage outliers. 
#       The worst outliers are automatically labeled by rownum in the plot. If the outliers are 
#       far outside the 2 standard deviations then remove those rows from your data set, renumber 
#       the #rows of the resulting dataframe so there are no gaps in the integer row numbers, and 
#       then re-run your model to see if it improves.

# normal distribution, and uniform distribution of residuals
# no significant leverage points

#diagnostic 4.  generate Standardized Residual Plots against each predictor
par(mfrow=c(2,2))
StanRest <- rstandard(m1)
plot(p.Walks,StanRest,ylab="Standardized Residuals")
plot(b.Singles,StanRest,ylab="Standardized Residuals")
plot(b.Doubles,StanRest,ylab="Standardized Residuals")
plot(m1$fitted.values,StanRest,ylab="Standardized Residuals",xlab="Fitted Values")

#Examine plots for constant variability of residuals across ALL of the values of your predictor. 
#       Constant variability means residuals are disbursed uniformly across all predictor variable's values. 
#       You should not see any predictable pattern or model lacks constant variability and isn't valid.

# uniform distribution of residuals

#diagnostic 5.  generate plot of Y "response variable"" against Fitted Values "regression model"
par(mfrow = c(2,2))
plot(m1$fitted.values,Wins,xlab="Fitted Values",ylab=expression(Wins^lambda))
abline(lsfit(m1$fitted.values,Wins))
plot(m1)
#If plot doesn't shows a linear relationship with no pattern or skew the model lacks normality.

# normal distribution, and uniform distribution of residuals
```

#####Added Stolen Bases and Double Plays
\[
\begin{aligned}
\widehat{wins} &= \hat{\beta}_0 + 
                    \hat{\beta}_1 \times p.Walks 
                    \hat{\beta}_2 \times b.Singles + 
                    \hat{\beta}_3 \times b.Doubles +
                    \hat{\beta}_4 \times b.Stolen Bases +                    
                    \end{aligned}
\]
\[
\begin{aligned}
                    \hat{\beta}_5 \times f.Double Plays +                    
                    \end{aligned}
\]

```{r m1-interation4,eval=TRUE,include=FALSE}
#VARIABLES
#variables have been transformed first as individual predictors
Wins  <- mW
p.Walks <- tmp.BB
b.Singles <- tmb.1B
b.Doubles <- tmb.2B
b.StolenBases <- tmb.SB
f.DoublePlays <- tmf.DP
m1 <- lm(Wins ~ p.Walks+b.Singles+b.Doubles+b.StolenBases+f.DoublePlays)

#PAIRWISE PLOT
par(mfrow=c(1,1))
pairs(Wins ~ p.Walks+b.Singles+b.Doubles+b.StolenBases+f.DoublePlays)

#MODEL DIAGNOSTICS
summary(m1)

#diagnostic 1. show collinearity of variables after checking p-values to < 0.05.
vif(m1)
#If resulting coefficients are > 5, remove the variable w/ largest VIF and re-run the model. 
#Subsequently remove variables either due to p-values becoming > 0.05 or VIF coefficients > 5. 
#Continue until all remaining p-values are < 0.05 and no VIFs  > 5

#p-values are all < 0.05 and no VIFs  > 5

#diagnostic 2.  generate Added Variable Plots: should show linear relationship between response & predictors:
par(mfrow=c(2,2))
avPlots(m1, ~.,ask=FALSE, id.n = 2)

#relationship is linear

#diagnostic 3.  generate Summary Diagnostic Plots
par(mfrow=c(2,2))
plot(m1)
#Upper Left plot "Residuals vs Fitted" 
#       Is there a clear predictable pattern in that plot?  If no model isn't valid
#       Do residuals have uniform variability for all fitted values?  If no model isn't valid.
#Upper Right 
#       Is there normality in your residuals? If not, the model lack normality and the model isn't valid.
#Lower Right plot "Residuals vs. Leverage"
#       The Y-axis in this plot represents standard deviations from the mean of the #residuals. 
#       If most residuals are within 2 standard deviations of mean, the model is likely valid. 
#       The plot also shows #"Cooks Distance" -- a metric for identifying high leverage outliers. 
#       The worst outliers are automatically labeled by rownum in the plot. If the outliers are 
#       far outside the 2 standard deviations then remove those rows from your data set, renumber 
#       the #rows of the resulting dataframe so there are no gaps in the integer row numbers, and 
#       then re-run your model to see if it improves.

# normal distribution, and uniform distribution of residuals
# no significant leverage points

#diagnostic 4.  generate Standardized Residual Plots against each predictor
par(mfrow=c(2,2))
StanRest <- rstandard(m1)
plot(p.Walks,StanRest,ylab="Standardized Residuals")
plot(b.Singles,StanRest,ylab="Standardized Residuals")
plot(b.Doubles,StanRest,ylab="Standardized Residuals")
plot(b.StolenBases,StanRest,ylab="Standardized Residuals")
plot(f.DoublePlays,StanRest,ylab="Standardized Residuals")
plot(m1$fitted.values,StanRest,ylab="Standardized Residuals",xlab="Fitted Values")

#Examine plots for constant variability of residuals across ALL of the values of your predictor. 
#       Constant variability means residuals are disbursed uniformly across all predictor variable's values. 
#       You should not see any predictable pattern or model lacks constant variability and isn't valid.

# uniform distribution of residuals

#diagnostic 5.  generate plot of Y "response variable"" against Fitted Values "regression model"
par(mfrow = c(2,2))
plot(m1$fitted.values,Wins,xlab="Fitted Values",ylab=expression(Wins^lambda))
abline(lsfit(m1$fitted.values,Wins))
plot(m1)
#If plot doesn't shows a linear relationship with no pattern or skew the model lacks normality.

# normal distribution, and uniform distribution of residuals
```

#####Added b.Walks and p.Strikeouts
\[
\begin{aligned}
\widehat{wins} &= \hat{\beta}_0 + 
                    \hat{\beta}_1 \times p.Walks 
                    \hat{\beta}_2 \times b.Singles + 
                    \hat{\beta}_3 \times b.Doubles +
                    \hat{\beta}_4 \times b.Stolen Bases +                    
                    \end{aligned}
\]
\[
\begin{aligned}
                    \hat{\beta}_5 \times f.Double Plays +   
                    \hat{\beta}_6 \times p.Walks +                    
                    \hat{\beta}_7 \times p.Strike Outs +                    
                    \end{aligned}
\]


```{r m1-interation5,eval=TRUE,include=FALSE}
#VARIABLES
#variables have been transformed first as individual predictors
Wins  <- mW
p.Walks <- tmp.BB
b.Singles <- tmb.1B
b.Doubles <- tmb.2B
b.StolenBases <- tmb.SB
f.DoublePlays <- tmf.DP
b.Walks <- tmb.BB
p.StrikeOuts <- tmp.SO
m1 <- lm(Wins ~ p.Walks+b.Singles+b.Doubles+b.StolenBases+f.DoublePlays+b.Walks+p.StrikeOuts)

#PAIRWISE PLOT
par(mfrow=c(1,1))
pairs(Wins ~ p.Walks+b.Singles+b.Doubles+b.StolenBases+f.DoublePlays+b.Walks+p.StrikeOuts)

#MODEL DIAGNOSTICS
summary(m1)

#diagnostic 1. show collinearity of variables after checking p-values to < 0.05.
vif(m1)
#If resulting coefficients are > 5, remove the variable w/ largest VIF and re-run the model. 
#Subsequently remove variables either due to p-values becoming > 0.05 or VIF coefficients > 5. 
#Continue until all remaining p-values are < 0.05 and no VIFs  > 5

#p-values are all < 0.05 but VIFs  > 5
#highest vif is b.Walks so it gets removed
```

#####Remove b.Walks
\[
\begin{aligned}
\widehat{wins} &= \hat{\beta}_0 + 
                    \hat{\beta}_1 \times p.Walks 
                    \hat{\beta}_2 \times b.Singles + 
                    \hat{\beta}_3 \times b.Doubles +
                    \hat{\beta}_4 \times b.Stolen Bases +                    
                    \end{aligned}
\]
\[
\begin{aligned}
                    \hat{\beta}_5 \times f.Double Plays +  
                    \hat{\beta}_6 \times p.Strike Outs +                    
                    \end{aligned}
\]

```{r m1-interation6,eval=TRUE,include=FALSE}
#VARIABLES
#variables have been transformed first as individual predictors
Wins  <- mW
p.Walks <- tmp.BB
b.Singles <- tmb.1B
b.Doubles <- tmb.2B
b.StolenBases <- tmb.SB
f.DoublePlays <- tmf.DP
p.StrikeOuts <- tmp.SO
m1 <- lm(Wins ~ p.Walks+b.Singles+b.Doubles+b.StolenBases+f.DoublePlays+p.StrikeOuts)

#PAIRWISE PLOT
par(mfrow=c(1,1))
pairs(Wins ~ p.Walks+b.Singles+b.Doubles+b.StolenBases+f.DoublePlays+p.StrikeOuts)

#MODEL DIAGNOSTICS
summary(m1)

#diagnostic 1. show collinearity of variables after checking p-values to < 0.05.
vif(m1)
#If resulting coefficients are > 5, remove the variable w/ largest VIF and re-run the model. 
#Subsequently remove variables either due to p-values becoming > 0.05 or VIF coefficients > 5. 
#Continue until all remaining p-values are < 0.05 and no VIFs  > 5

#p-value for b.Walks > 0.05 all VIFs < 5

```

#####Add b.StrikeOuts and b.Triples
\[
\begin{aligned}
\widehat{wins} &= \hat{\beta}_0 + 
                    \hat{\beta}_1 \times p.Walks 
                    \hat{\beta}_2 \times b.Singles + 
                    \hat{\beta}_3 \times b.Doubles +
                    \hat{\beta}_4 \times b.Stolen Bases +                    
                    \end{aligned}
\]
\[
\begin{aligned}
                    \hat{\beta}_5 \times f.Double Plays +  
                    \hat{\beta}_6 \times b.Strike Outs +
                    \hat{\beta}_7 \times b.Triples +                                      
                    \end{aligned}
\]

```{r m1-interation7,eval=TRUE}
#VARIABLES
#variables have been transformed first as individual predictors
Wins  <- mW
p.Walks <- tmp.BB
b.Singles <- tmb.1B
b.Doubles <- tmb.2B
b.StolenBases <- tmb.SB
f.DoublePlays <- tmf.DP
b.StrikeOuts <- tmb.SO
b.Triples <- tmb.3B
m1 <- lm(Wins ~ p.Walks+b.Singles+b.Doubles+b.StolenBases+f.DoublePlays+b.StrikeOuts+b.Triples)

#PAIRWISE PLOT
par(mfrow=c(1,1))
pairs(Wins ~ p.Walks+b.Singles+b.Doubles+b.StolenBases+b.StrikeOuts+b.Triples)

#MODEL DIAGNOSTICS
summary(m1)

#diagnostic 1. show collinearity of variables after checking p-values to < 0.05.
vif(m1)

#If resulting coefficients are > 5, remove the variable w/ largest VIF and re-run the model. 
#Subsequently remove variables either due to p-values becoming > 0.05 or VIF coefficients > 5. 
#Continue until all remaining p-values are < 0.05 and no VIFs  > 5

#p-values are all < 0.05 and no VIFs  > 5

#diagnostic 2.  generate Added Variable Plots: should show linear relationship between response & predictors:
par(mfrow=c(2,2))
avPlots(m1, ~.,ask=FALSE, id.n = 2)

#relationship is linear

#diagnostic 3.  generate Summary Diagnostic Plots
par(mfrow=c(2,2))
plot(m1)
#Upper Left plot "Residuals vs Fitted" 
#       Is there a clear predictable pattern in that plot?  If no model isn't valid
#       Do residuals have uniform variability for all fitted values?  If no model isn't valid.
#Upper Right 
#       Is there normality in your residuals? If not, the model lack normality and the model isn't valid.
#Lower Right plot "Residuals vs. Leverage"
#       The Y-axis in this plot represents standard deviations from the mean of the #residuals. 
#       If most residuals are within 2 standard deviations of mean, the model is likely valid. 
#       The plot also shows #"Cooks Distance" -- a metric for identifying high leverage outliers. 
#       The worst outliers are automatically labeled by rownum in the plot. If the outliers are 
#       far outside the 2 standard deviations then remove those rows from your data set, renumber 
#       the #rows of the resulting dataframe so there are no gaps in the integer row numbers, and 
#       then re-run your model to see if it improves.

# normal distribution, and uniform distribution of residuals
# no significant leverage points

#diagnostic 4.  generate Standardized Residual Plots against each predictor
par(mfrow=c(2,2))
StanRest <- rstandard(m1)
plot(p.Walks,StanRest,ylab="Standardized Residuals")
plot(b.Singles,StanRest,ylab="Standardized Residuals")
plot(b.Doubles,StanRest,ylab="Standardized Residuals")
plot(b.StolenBases,StanRest,ylab="Standardized Residuals")
plot(f.DoublePlays,StanRest,ylab="Standardized Residuals")
plot(b.StrikeOuts,StanRest,ylab="Standardized Residuals")
plot(b.Triples,StanRest,ylab="Standardized Residuals")
plot(m1$fitted.values,StanRest,ylab="Standardized Residuals",xlab="Fitted Values")

#Examine plots for constant variability of residuals across ALL of the values of your predictor. 
#       Constant variability means residuals are disbursed uniformly across all predictor variable's values. 
#       You should not see any predictable pattern or model lacks constant variability and isn't valid.

# uniform distribution of residuals

#diagnostic 5.  generate plot of Y "response variable"" against Fitted Values "regression model"
par(mfrow = c(2,2))
plot(m1$fitted.values,Wins,xlab="Fitted Values",ylab=expression(Wins^lambda))
abline(lsfit(m1$fitted.values,Wins))
plot(m1)
#If plot doesn't shows a linear relationship with no pattern or skew the model lacks normality.

# normal distribution, and uniform distribution of residuals
```